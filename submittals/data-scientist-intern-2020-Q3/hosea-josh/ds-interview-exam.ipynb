{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"clearfix\" style=\"padding: 10px; padding-left: 0px\">\n",
    "<a href=\"http://bombora.com\"><img src=\"https://app.box.com/shared/static/e0j9v1xjmubit0inthhgv3llwnoansjp.png\" width=\"200px\" class=\"pull-right\" style=\"display: inline-block; margin: 5px; vertical-align: middle;\"></a>\n",
    "<h1> Bombora Data Science: <br> *Interview Exam* </h1>\n",
    "</div>\n",
    "\n",
    "<img width=\"200px\" src=\"https://app.box.com/shared/static/15slg1mvjd1zldbg3xkj9picjkmhzpa5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Welcome\n",
    "\n",
    "Welcome! This notebook contains interview exam questions referenced in the *Instructions* section in the `README.md`â€”please read that first, *before* attempting to answer questions here.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p style=\"font-weight:bold\">ADVICE</p>\n",
    "<p>*Do not* read these questions, and panic, *before* reading the instructions in `README.md`.</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p style=\"font-weight:bold\">WARNING</p>\n",
    "\n",
    "<p>If using <a href=\"https://try.jupyter.org\">try.jupyter.org</a> do not rely on the server for anything you want to last - your server will be <span style=\"font-weight:bold\">deleted after 10 minutes of inactivity</span>. Save often and rember download notebook when you step away (you can always re-upload and start again)!</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Have fun!\n",
    "\n",
    "Regardless of outcome, getting to know you is important. Give it your best shot and we'll look forward to following up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algo + Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.1: Fibionacci\n",
    "![fib image](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Fibonacci_spiral_34.svg/200px-Fibonacci_spiral_34.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.1\n",
    "Given $n$ where $n \\in \\mathbb{N}$ (i.e., $n$ is an integer and $n > 0$), write a function `fibonacci(n)` that computes the Fibonacci number $F_n$, where $F_n$ is defined by the recurrence relation:\n",
    "\n",
    "$$ F_n = F_{n-1} + F_{n-2}$$\n",
    "\n",
    "with initial conditions of:\n",
    "\n",
    "$$ F_1 = 1,  F_2 = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def fibonacci(n):\n",
    "    \n",
    "    alpha = (1+sqrt(5))/2\n",
    "    beta = (1-sqrt(5))/2\n",
    "    \n",
    "    return int((alpha**n - beta**n)/sqrt(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1\n",
      "2: 1\n",
      "3: 2\n",
      "4: 3\n",
      "5: 5\n",
      "6: 8\n",
      "7: 13\n",
      "8: 21\n",
      "9: 34\n",
      "10: 55\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    print(f\"{i}: {fibonacci(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.2\n",
    "What's the complexity of your implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "\n",
    "The time complexity of this version of the fibonacci function is constant, O(1). The space complexity is also constant, O(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.3\n",
    "Consider an alternative implementation to compute Fibonacci number $F_n$ and write a new function, `fibonacci2(n)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci2(n):\n",
    "    \n",
    "    if(n <= 2):\n",
    "        return 1\n",
    "    \n",
    "    n_1 = 1\n",
    "    n_2 = 1\n",
    "    \n",
    "    for i in range(3,n+1):\n",
    "        fib = n_1 + n_2\n",
    "        n_2 = n_1\n",
    "        n_1 = fib\n",
    "    \n",
    "    return fib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1\n",
      "2: 1\n",
      "3: 2\n",
      "4: 3\n",
      "5: 5\n",
      "6: 8\n",
      "7: 13\n",
      "8: 21\n",
      "9: 34\n",
      "10: 55\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    print(f\"{i}: {fibonacci2(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.4\n",
    "What's the complexity of your implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "The time complexity of this implementation is O(n). The space complexity is constant, O(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.5\n",
    "What are some examples of optimizations that could improve computational performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "The second fibonacci function could only be improved if we used an equation as in the first function to calculate the nth number in constant time or if there were a way to calculate it in log(n) time. Given the format of the second function as it is, there isn't an obvious way to reduce the space complexity either. In the first fibonacci function, although it runs in constant time the performance could be improved, although to an almost unnoticable level, by removing the assignment of alpha and beta and including their actual values in the return statement. The space complexity would also improve, although again the improvement would be extremely minimally as it is already constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prob + Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.2: Making a 6-side die roll a 7?\n",
    "\n",
    "Using a single 6-side die, how can you generate a random number between 1 - 7?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Josh's Very Special Unique Answer:\n",
    "Essentially we are going to exapnd the space to a higher dimension and then map the expanded space to 7 outcomes. Using a 6 sided dice, if you rolled the dice twice to create the ordered pair (roll_1, roll_2), there would be 36 possible outcomes each with equal probability. If you were to not consider (effectively removing) the ordered pair where roll_1 = roll_2 = 6, then there would be 35 possible outcomes. If you then split the 35 outcomes into 7 groups of 5, each will have a 5/36 chance of occuring. Then you could assign each grouping a number from 1 to 7. After rolling the dice twice to create the ordered set and you return the number of the grouping the set is in. If the pair is the excluded pair then no number is returned and you must roll again. Below are example groupings:\n",
    "\n",
    "### Excluded Pair:  Return Nothing\n",
    "- (6,6)\n",
    "\n",
    "### Set 1: Return 1\n",
    "- (1,1)\n",
    "- (1,2)\n",
    "- (1,3)\n",
    "- (1,4)\n",
    "- (1,5)\n",
    "\n",
    "### Set 2: Return 2\n",
    "- (2,1)\n",
    "- (2,2)\n",
    "- (2,3)\n",
    "- (2,4)\n",
    "- (2,5)\n",
    "\n",
    "### Set 3: Return 3\n",
    "- (3,1)\n",
    "- (3,2)\n",
    "- (3,3)\n",
    "- (3,4)\n",
    "- (3,5)\n",
    "\n",
    "### Set 4: Return 4\n",
    "- (4,1)\n",
    "- (4,2)\n",
    "- (4,3)\n",
    "- (4,4)\n",
    "- (4,5)\n",
    "\n",
    "### Set 5: Return 5\n",
    "- (5,1)\n",
    "- (5,2)\n",
    "- (5,3)\n",
    "- (5,4)\n",
    "- (5,5)\n",
    "\n",
    "### Set 6: Return 6\n",
    "- (6,1)\n",
    "- (6,2)\n",
    "- (6,3)\n",
    "- (6,4)\n",
    "- (6,5)\n",
    "\n",
    "### Set 7: Return 7\n",
    "- (1,6)\n",
    "- (2,6)\n",
    "- (3,6)\n",
    "- (4,6)\n",
    "- (5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conceptual ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3.1 Why study gradient boosting or neural networks?\n",
    "\n",
    "Consider a regression setting where $X \\in \\mathbb{R}^p$ and $Y \\in \\mathbb{R}$. The goal is to come up with a function $f(X): \\mathbb{R}^p \\rightarrow \\mathbb{R}$ that minimizes the squared-error loss $(Y - f(X))^2$. Since X, Y are random variables, we seek to minimize the expectation of the squared error loss as follows\n",
    "\\begin{equation}\n",
    "EPE(f) = \\mathbb{E}\\left[(Y-f(X)^2\\right]\n",
    "\\end{equation}\n",
    "where EPE stands for expected prediction error. One can show that minimizing the expected prediction error leads to the following _regression function_\n",
    "\\begin{equation}\n",
    "f(x) = \\mathbb{E}\\left[Y|X=x\\right]\n",
    "\\end{equation}\n",
    "\n",
    "The goal of any method is to approximate the regression function above, which we denote as $\\hat{f}(x)$. For example, linear regression explicitly assumes that the regression function is approximately linear in its arguments, i.e. $\\hat{f}(x) = x^T\\beta$ while a neural network provides a nonlinear approximation of the regression function. \n",
    "\n",
    "The simplest of all these methods is [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). Given $x$ and some neighbourhood of $k$ points $N_k(x)$, $\\hat{f}(x)$ is simply the average of all $y_i|x_i \\in N_k(x)$.  Let $N$ denote the training sample size. Under mild regularity conditions on the joint probability distribution $Pr(X, Y)$, one can show that as $N \\rightarrow \\infty$, $k \\rightarrow \\infty$ such that $k/N \\rightarrow 0$, then $\\hat{f}(x) \\rightarrow f(x)$ where $\\rightarrow$ means approaches or goes to. In other words, the k-nearest neighbors algorithm converges to the ideal solution as both the training sample size and number of neighbors increase to infinity.\n",
    "\n",
    "Now given this _universal approximator_, why look any further and research other methods? Please share your thoughts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: \n",
    "There are several reasons to continue researching other methods. Firstly, for KNN we need to find a good value for k in order to avoid both underfitting and overfitting on the training data. With small k values, KNN can also be especially sensitive to outliers and noise in the training data while larger k values are computationally expensive. KNN is also very memory intensive as the model itself is the training data, whereas other models require less memory. Neural networks, on the other hand, are made up of the weight and bias matrices and K-Means is only the cluster centers. When making predicitions, KNN is also computationally more expensive than other methods as it has to search the dataset for the closest neighbors. While KNN may converge to the ideal solution given $N \\rightarrow \\infty$ and $k \\rightarrow \\infty$, there is a high cost for using such a model as both the memory requirement and computational cost increase significatly to support the higher k value and larger dataset. Thus, it is worth it to invest in other methods that may take up less memory and are computationally less expensive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
